{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T11:28:01.192634Z",
     "start_time": "2025-10-29T11:28:00.484226Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\dl\\lib\\site-packages\\jieba\\_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "70c532e6666be163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:28:17.284793Z",
     "start_time": "2025-10-29T11:28:15.152827Z"
    }
   },
   "source": [
    "train_df=pd.read_csv('../data/processed/train.csv')\n",
    "test_df=pd.read_csv('../data/processed/test.csv')\n",
    "val_df=pd.read_csv('../data/processed/val.csv')\n",
    "print(f'è®­ç»ƒé›†{train_df.shape}')\n",
    "print(f'æµ‹è¯•é›†{test_df.shape}')\n",
    "print(f'éªŒè¯é›†{val_df.shape}')\n",
    "print(f\"\\nç±»åˆ«: {train_df['label'].unique()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒé›†(45462, 4)\n",
      "æµ‹è¯•é›†(9743, 4)\n",
      "éªŒè¯é›†(9742, 4)\n",
      "\n",
      "ç±»åˆ«: [13  9  6  7  3  0 10  2  5 12  4  1 11  8]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1388b9f15c7afacc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:28:26.295830Z",
     "start_time": "2025-10-29T11:28:26.284768Z"
    }
   },
   "source": [
    "# ä¸­æ–‡åœç”¨è¯è¡¨\n",
    "stopwords = set([\n",
    "    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½',\n",
    "    'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š',\n",
    "    'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'ä¸­', 'ä»¥', 'æ¥', 'ä¸ª',\n",
    "    'åœ°', 'ä¸º', 'ä»–', 'å¾—', 'å¥¹', 'å¯¹', 'ä¹ˆ', 'é‡Œ', 'å', 'èƒ½', 'å†',\n",
    "    'è€Œ', 'è¢«', 'ä»', 'æŠŠ', 'è®©', 'ä¸', 'ç­‰', 'åˆ«', 'ä¹‹', 'è¿™ä¸ª',\n",
    "])\n",
    "\n",
    "print(f\"åœç”¨è¯æ•°é‡: {len(stopwords)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åœç”¨è¯æ•°é‡: 53\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "5ac5f6ce3aeb6f49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:31:23.498924Z",
     "start_time": "2025-10-29T11:31:22.271800Z"
    }
   },
   "source": [
    "def tokenize(text):\n",
    "    words=jieba.cut(text)\n",
    "    words=[w for w in words if w not in stopwords]\n",
    "    return words\n",
    "# æµ‹è¯•åˆ†è¯\n",
    "sample_text = train_df.iloc[0]['text']\n",
    "print(\"åŸæ–‡:\")\n",
    "print(sample_text[:100])\n",
    "print(\"\\nåˆ†è¯å:\")\n",
    "print(\" / \".join(tokenize(sample_text)[:20]))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ysn\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸæ–‡:\n",
      "æˆé•¿å‹åŸºé‡‘çŒœæƒ³2009ï¼šå¹¸ç¦å²æœˆèƒ½å¦å»¶ç»­\n",
      "ã€€ã€€å®ä¹ è®°è€… æ¨é¢–æ¡¦ æœ¬æŠ¥è®°è€… å¼ æ¡” \n",
      "ã€€ã€€ç©¶ç«Ÿæ˜¯å…¨å¸‚åœºåŸºé‡‘è¿˜æ˜¯æˆé•¿å‹åŸºé‡‘ä¼šåœ¨2009å¹´èµ°ç‰›ï¼Ÿç§ç§è¿¹è±¡æ˜¾ç¤ºç­”æ¡ˆæ˜¯åè€…ã€‚ \n",
      "ã€€ã€€é“¶æ²³è¯åˆ¸åŸºé‡‘ç ”ç©¶ä¸­å¿ƒçš„ç»Ÿè®¡æ˜¾ç¤ºï¼Œ\n",
      "\n",
      "åˆ†è¯å:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.215 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆé•¿å‹ / åŸºé‡‘ / çŒœæƒ³ / 2009 / ï¼š / å¹¸ç¦ / å²æœˆ / èƒ½å¦ / å»¶ç»­ / \n",
      " / ã€€ / ã€€ / å®ä¹  / è®°è€… /   / æ¨é¢–æ¡¦ /   / æœ¬æŠ¥è®°è€… /   / å¼ æ¡”\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "1e143489a8077c91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:35:21.922886Z",
     "start_time": "2025-10-29T11:31:54.985177Z"
    }
   },
   "source": [
    "# å¯¹æ‰€æœ‰æ•°æ®åˆ†è¯\n",
    "from tqdm import tqdm\n",
    "\n",
    "# é‡è¦ï¼å…ˆå¯ç”¨pandasçš„è¿›åº¦æ¡\n",
    "tqdm.pandas()\n",
    "train_df['words']=train_df['text'].progress_apply(tokenize)\n",
    "test_df['words']=test_df['text'].progress_apply(tokenize)\n",
    "val_df['words']=val_df['text'].progress_apply(tokenize)\n",
    "\n",
    "print(\"\\n åˆ†è¯å®Œæˆï¼\")\n",
    "\n",
    "# æŸ¥çœ‹åˆ†è¯åçš„ç»“æœ\n",
    "print(\"\\nç¤ºä¾‹ï¼ˆå‰3æ¡ï¼‰:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nç¬¬{i+1}æ¡:\")\n",
    "    print(f\"æ ‡ç­¾: {train_df.iloc[i]['label']}\")\n",
    "    print(f\"åˆ†è¯: {' '.join(train_df.iloc[i]['words'][:15])}...\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45462/45462 [02:26<00:00, 310.68it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9743/9743 [00:28<00:00, 341.82it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9742/9742 [00:32<00:00, 303.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " åˆ†è¯å®Œæˆï¼\n",
      "\n",
      "ç¤ºä¾‹ï¼ˆå‰3æ¡ï¼‰:\n",
      "\n",
      "ç¬¬1æ¡:\n",
      "æ ‡ç­¾: 13\n",
      "åˆ†è¯: æˆé•¿å‹ åŸºé‡‘ çŒœæƒ³ 2009 ï¼š å¹¸ç¦ å²æœˆ èƒ½å¦ å»¶ç»­ \n",
      " ã€€ ã€€ å®ä¹  è®°è€…  ...\n",
      "\n",
      "ç¬¬2æ¡:\n",
      "æ ‡ç­¾: 9\n",
      "åˆ†è¯: ã€Š ä¾ ç›— è½¦æ‰‹ ã€‹ å°† æ¨å‡º ç»­ä½œ   æ˜å¹´ äº®ç›¸ E3 \n",
      " ã€€ ã€€ ç”±...\n",
      "\n",
      "ç¬¬3æ¡:\n",
      "æ ‡ç­¾: 6\n",
      "åˆ†è¯: ç»„å›¾ ï¼š 6 æ¬¾ æ€§æ„Ÿ ä¸è¢œ æœ€é… å¤å­£ è¶…çŸ­ \n",
      " ã€€ ã€€ å¯¼è¯» ï¼š ç©¿...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "ac6a8f86c1878870",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:35:48.292262Z",
     "start_time": "2025-10-29T11:35:44.574424Z"
    }
   },
   "source": [
    "all_words=[]\n",
    "for word in train_df['words']:\n",
    "    all_words.extend(word)\n",
    "    \n",
    "word_freq=Counter(all_words)\n",
    "print(f\"æ€»è¯: {len(all_words)}\")\n",
    "print(f\"ä¸é‡å¤è¯æ•°:{word_freq}\")\n",
    "\n",
    "min_freq=5\n",
    "word_freq={word:freq for word, freq in word_freq.items() if freq>=min_freq}\n",
    "print(f\"ä¿ç•™è¯æ•°: {len(word_freq)}\")\n",
    "\n",
    "sorted_words=sorted(word_freq.items(), key=lambda x:x[1], reverse=True)\n",
    "# æ„å»ºè¯åˆ°IDçš„æ˜ å°„\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}  # ç‰¹æ®Štoken\n",
    "for word, freq in sorted_words:\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"\\nè¯è¡¨å¤§å°: {len(vocab):,}\")\n",
    "\n",
    "print(f\"\\nTop 20 é«˜é¢‘è¯:\")\n",
    "for i, (word, freq) in enumerate(sorted_words[:20], 1):\n",
    "    print(f\"{i:2d}. {word}: {freq:,}\")\n",
    "\n",
    "# ä¿å­˜è¯è¡¨\n",
    "with open('../data/processed/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "print(\"\\n è¯è¡¨å·²ä¿å­˜åˆ° data/processed/vocab.pkl\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ€»è¯: 23111713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " è¯è¡¨å·²ä¿å­˜åˆ° data/processed/vocab.pkl\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "1c7145cbb1c4d50e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:37:33.645371Z",
     "start_time": "2025-10-29T11:37:29.090157Z"
    }
   },
   "source": [
    "# Cell 7: æ–‡æœ¬è½¬IDåºåˆ—\n",
    "print(\"å°†æ–‡æœ¬è½¬æ¢ä¸ºIDåºåˆ—...\")\n",
    "\n",
    "def words_to_ids(words, vocab, max_len=512):\n",
    "    \"\"\"å°†è¯åˆ—è¡¨è½¬æ¢ä¸ºIDåˆ—è¡¨\"\"\"\n",
    "    ids = [vocab.get(word, vocab['<UNK>']) for word in words]\n",
    "\n",
    "    if len(ids) > max_len:\n",
    "        ids = ids[:max_len]\n",
    "    else:\n",
    "        ids = ids + [vocab['<PAD>']] * (max_len - len(ids))\n",
    "\n",
    "    return ids\n",
    "\n",
    "# è½¬æ¢\n",
    "train_df['ids'] = train_df['words'].apply(lambda x: words_to_ids(x, vocab))\n",
    "val_df['ids'] = val_df['words'].apply(lambda x: words_to_ids(x, vocab))\n",
    "test_df['ids'] = test_df['words'].apply(lambda x: words_to_ids(x, vocab))\n",
    "\n",
    "print(\"IDè½¬æ¢å®Œæˆï¼\")\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "idx = 0\n",
    "print(f\"\\nç¤ºä¾‹:\")\n",
    "print(f\"åŸæ–‡: {train_df.iloc[idx]['text'][:50]}...\")\n",
    "print(f\"åˆ†è¯: {' '.join(train_df.iloc[idx]['words'][:10])}...\")\n",
    "print(f\"IDåºåˆ—å‰20ä¸ª: {train_df.iloc[idx]['ids'][:20]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å°†æ–‡æœ¬è½¬æ¢ä¸ºIDåºåˆ—...\n",
      "IDè½¬æ¢å®Œæˆï¼\n",
      "\n",
      "ç¤ºä¾‹:\n",
      "åŸæ–‡: æˆé•¿å‹åŸºé‡‘çŒœæƒ³2009ï¼šå¹¸ç¦å²æœˆèƒ½å¦å»¶ç»­\n",
      "ã€€ã€€å®ä¹ è®°è€… æ¨é¢–æ¡¦ æœ¬æŠ¥è®°è€… å¼ æ¡” \n",
      "ã€€ã€€ç©¶ç«Ÿæ˜¯å…¨å¸‚åœºåŸº...\n",
      "åˆ†è¯: æˆé•¿å‹ åŸºé‡‘ çŒœæƒ³ 2009 ï¼š å¹¸ç¦ å²æœˆ èƒ½å¦ å»¶ç»­ \n",
      "...\n",
      "IDåºåˆ—å‰20ä¸ª: [29558, 34, 9530, 151, 10, 1720, 5850, 1432, 1596, 5, 3, 3, 2944, 45, 6, 1, 6, 1160, 6, 89093]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "ac3ed2fd1e7e778a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:39:28.660121Z",
     "start_time": "2025-10-29T11:39:28.646884Z"
    }
   },
   "source": [
    "# Cell 8: ä¿å­˜æ ‡ç­¾æ˜ å°„\n",
    "import pickle\n",
    "\n",
    "# THUCNews 14ä¸ªç±»åˆ«\n",
    "label_map_save = {\n",
    "    0: 'ä½“è‚²', 1: 'å¨±ä¹', 2: 'å®¶å±…', 3: 'å½©ç¥¨', 4: 'æˆ¿äº§', 5: 'æ•™è‚²',\n",
    "    6: 'æ—¶å°š', 7: 'æ—¶æ”¿', 8: 'æ˜Ÿåº§', 9: 'æ¸¸æˆ', 10: 'ç¤¾ä¼š', 11: 'ç§‘æŠ€',\n",
    "    12: 'è‚¡ç¥¨', 13: 'è´¢ç»'\n",
    "}\n",
    "\n",
    "with open('../data/processed/label_map.pkl', 'wb') as f:\n",
    "    pickle.dump(label_map_save, f)\n",
    "\n",
    "print(\"æ ‡ç­¾æ˜ å°„å·²ä¿å­˜\")\n",
    "print(\"\\næ ‡ç­¾æ˜ å°„:\")\n",
    "for k, v in label_map_save.items():\n",
    "    print(f\"{k:2d} â†’ {v}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ‡ç­¾æ˜ å°„å·²ä¿å­˜\n",
      "\n",
      "æ ‡ç­¾æ˜ å°„:\n",
      " 0 â†’ ä½“è‚²\n",
      " 1 â†’ å¨±ä¹\n",
      " 2 â†’ å®¶å±…\n",
      " 3 â†’ å½©ç¥¨\n",
      " 4 â†’ æˆ¿äº§\n",
      " 5 â†’ æ•™è‚²\n",
      " 6 â†’ æ—¶å°š\n",
      " 7 â†’ æ—¶æ”¿\n",
      " 8 â†’ æ˜Ÿåº§\n",
      " 9 â†’ æ¸¸æˆ\n",
      "10 â†’ ç¤¾ä¼š\n",
      "11 â†’ ç§‘æŠ€\n",
      "12 â†’ è‚¡ç¥¨\n",
      "13 â†’ è´¢ç»\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "7f8bbe1a37989281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T11:40:50.103083Z",
     "start_time": "2025-10-29T11:40:19.482115Z"
    }
   },
   "source": [
    "# Cell 9: ä¿å­˜æœ€ç»ˆæ•°æ®å¹¶æ€»ç»“\n",
    "import pickle\n",
    "\n",
    "print(\"ä¿å­˜æœ€ç»ˆå¤„ç†ç»“æœ...\")\n",
    "\n",
    "# ä¿å­˜å®Œæ•´æ•°æ®ï¼ˆåŒ…å«text, label, label_name, words, idsç­‰ï¼‰\n",
    "train_df.to_pickle('../data/processed/train_processed.pkl')\n",
    "val_df.to_pickle('../data/processed/val_processed.pkl')\n",
    "test_df.to_pickle('../data/processed/test_processed.pkl')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ æ•°æ®é¢„å¤„ç†å…¨éƒ¨å®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\næ•°æ®ç»Ÿè®¡:\")\n",
    "print(f\"  è®­ç»ƒé›†: {len(train_df):,} æ¡\")\n",
    "print(f\"  éªŒè¯é›†: {len(val_df):,} æ¡\")\n",
    "print(f\"  æµ‹è¯•é›†: {len(test_df):,} æ¡ ï¼ˆæœ‰labelï¼Œå¯ä»¥è¯„ä¼°ï¼‰\")\n",
    "print(f\"  è¯è¡¨å¤§å°: {len(vocab):,}\")\n",
    "print(f\"  ç±»åˆ«æ•°: 14\")\n",
    "print(f\"  åºåˆ—é•¿åº¦: 512\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¿å­˜æœ€ç»ˆå¤„ç†ç»“æœ...\n",
      "\n",
      "============================================================\n",
      "ğŸ‰ æ•°æ®é¢„å¤„ç†å…¨éƒ¨å®Œæˆï¼\n",
      "============================================================\n",
      "\n",
      "æ•°æ®ç»Ÿè®¡:\n",
      "  è®­ç»ƒé›†: 45,462 æ¡\n",
      "  éªŒè¯é›†: 9,742 æ¡\n",
      "  æµ‹è¯•é›†: 9,743 æ¡ ï¼ˆæœ‰labelï¼Œå¯ä»¥è¯„ä¼°ï¼‰\n",
      "  è¯è¡¨å¤§å°: 117,355\n",
      "  ç±»åˆ«æ•°: 14\n",
      "  åºåˆ—é•¿åº¦: 512\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "719a524a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ffd0c21302fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
