{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 8: BERTæµ‹è¯•å’Œç¯å¢ƒéªŒè¯\n",
    "\n",
    "æµ‹è¯•HuggingFace Transformersåº“ï¼Œç†Ÿæ‚‰BERTçš„åŠ è½½å’Œä½¿ç”¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:27:33.507569Z",
     "start_time": "2025-11-03T07:26:55.136252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformersåº“å¯¼å…¥æˆåŠŸ\n",
      "Transformersç‰ˆæœ¬: 4.57.1\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: å¯¼å…¥åº“\n",
    "\"\"\"\n",
    "HuggingFace Transformersåº“æ˜¯ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ ‡å‡†å·¥å…·\n",
    "æä¾›äº†BERTã€GPTç­‰å„ç§é¢„è®­ç»ƒæ¨¡å‹\n",
    "\"\"\"\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"Transformersåº“å¯¼å…¥æˆåŠŸ\")\n",
    "print(f\"Transformersç‰ˆæœ¬: {transformers.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:33:06.515648Z",
     "start_time": "2025-11-03T07:33:01.728231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½ bert-base-chinese çš„tokenizer...\n",
      "ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œä¼šä»HuggingFaceä¸‹è½½ï¼Œéœ€è¦å‡ åˆ†é’Ÿï¼‰\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849ecdb0295b4fdbbbd00f3f7bd6a3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\dl\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ysn\\.cache\\huggingface\\hub\\models--bert-base-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adb8671877e4c369b2f7d24b85728eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c47a71dd8d43f8bad508cde72f2717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a804122ab85741318095920569c0f0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenizeråŠ è½½æˆåŠŸ\n",
      "è¯è¡¨å¤§å°: 21,128\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: åŠ è½½BERT Tokenizer\n",
    "\"\"\"\n",
    "Tokenizerçš„ä½œç”¨ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºBERTèƒ½ç†è§£çš„æ ¼å¼\n",
    "- ä¸­æ–‡BERTä½¿ç”¨å­—ç¬¦çº§tokenizationï¼ˆæ¯ä¸ªæ±‰å­—æ˜¯ä¸€ä¸ªtokenï¼‰\n",
    "- ä¼šæ·»åŠ ç‰¹æ®Štokenï¼š[CLS]ã€[SEP]ã€[PAD]ç­‰\n",
    "\"\"\"\n",
    "\n",
    "model_name = 'bert-base-chinese'  # ä¸­æ–‡BERTé¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "print(f\"æ­£åœ¨åŠ è½½ {model_name} çš„tokenizer...\")\n",
    "print(\"ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œä¼šä»HuggingFaceä¸‹è½½ï¼Œéœ€è¦å‡ åˆ†é’Ÿï¼‰\")\n",
    "\n",
    "# åŠ è½½tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"TokenizeråŠ è½½æˆåŠŸ\")\n",
    "print(f\"è¯è¡¨å¤§å°: {tokenizer.vocab_size:,}\")  # BERTçš„è¯è¡¨å¤§å°ï¼ˆå›ºå®šçš„ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:37:48.256309Z",
     "start_time": "2025-11-03T07:37:48.213656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ–‡æœ¬: è¿™æ˜¯ä¸€æ¡ä½“è‚²æ–°é—»ï¼Œä¸­å›½ç”·ç¯®å–å¾—èƒœåˆ©\n",
      "==================================================\n",
      "\n",
      "ã€Step 1: åˆ†è¯ã€‘\n",
      "Tokens: ['è¿™', 'æ˜¯', 'ä¸€', 'æ¡', 'ä½“', 'è‚²', 'æ–°', 'é—»', 'ï¼Œ', 'ä¸­', 'å›½', 'ç”·', 'ç¯®', 'å–', 'å¾—', 'èƒœ', 'åˆ©']\n",
      "Tokenæ•°é‡: 17\n",
      "\n",
      "ã€Step 2: Tokenè½¬IDã€‘\n",
      "IDs: [6821, 3221, 671, 3340, 860, 5509, 3173, 7319, 8024, 704, 1744, 4511, 5074, 1357, 2533, 5526, 1164]\n",
      "\n",
      "ã€Step 3: ç›´æ¥encodeã€‘\n",
      "Encoded IDs: [101, 6821, 3221, 671, 3340, 860, 5509, 3173, 7319, 8024, 704, 1744, 4511, 5074, 1357, 2533, 5526, 1164, 102]\n",
      "é•¿åº¦: 19\n",
      "\n",
      "ã€Step 4: è§£ç ã€‘\n",
      "Decoded: [CLS] è¿™ æ˜¯ ä¸€ æ¡ ä½“ è‚² æ–° é—» ï¼Œ ä¸­ å›½ ç”· ç¯® å– å¾— èƒœ åˆ© [SEP]\n",
      " æ³¨æ„ï¼š\n",
      "  - [CLS]: å¥å­å¼€å§‹æ ‡è®°ï¼ˆID=101ï¼‰\n",
      "  - [SEP]: å¥å­ç»“æŸæ ‡è®°ï¼ˆID=102ï¼‰\n",
      "  - BERTçš„è¾“å…¥å¿…é¡»æœ‰è¿™ä¸¤ä¸ªç‰¹æ®Štoken\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: æµ‹è¯•Tokenizer - åŸºç¡€ç”¨æ³•\n",
    "\"\"\"\n",
    "ç†è§£tokenizerå¦‚ä½•å¤„ç†æ–‡æœ¬ï¼š\n",
    "1. æ–‡æœ¬ â†’ Tokenåºåˆ—ï¼ˆåˆ†è¯ï¼‰\n",
    "2. Token â†’ IDï¼ˆè½¬æ¢ä¸ºæ•°å­—ï¼‰\n",
    "3. æ·»åŠ ç‰¹æ®Štokenï¼š[CLS]ã€[SEP]\n",
    "\"\"\"\n",
    "\n",
    "# æµ‹è¯•æ–‡æœ¬\n",
    "text = \"è¿™æ˜¯ä¸€æ¡ä½“è‚²æ–°é—»ï¼Œä¸­å›½ç”·ç¯®å–å¾—èƒœåˆ©\"\n",
    "\n",
    "print(f\"åŸå§‹æ–‡æœ¬: {text}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# æ–¹æ³•1ï¼štokenizeï¼ˆåªåˆ†è¯ï¼Œè¿”å›tokenï¼‰\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"\\nã€Step 1: åˆ†è¯ã€‘\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Tokenæ•°é‡: {len(tokens)}\")\n",
    "\n",
    "# æ–¹æ³•2ï¼šconvert_tokens_to_idsï¼ˆtokenè½¬IDï¼‰\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"\\nã€Step 2: Tokenè½¬IDã€‘\")\n",
    "print(f\"IDs: {ids}\")\n",
    "\n",
    "# æ–¹æ³•3ï¼šencodeï¼ˆä¸€æ­¥åˆ°ä½ï¼šæ–‡æœ¬â†’IDï¼Œä¼šè‡ªåŠ¨æ·»åŠ [CLS]å’Œ[SEP]ï¼‰\n",
    "encoded = tokenizer.encode(text)\n",
    "print(f\"\\nã€Step 3: ç›´æ¥encodeã€‘\")\n",
    "print(f\"Encoded IDs: {encoded}\")\n",
    "print(f\"é•¿åº¦: {len(encoded)}\")\n",
    "\n",
    "# è§£ç ï¼šIDâ†’æ–‡æœ¬\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\nã€Step 4: è§£ç ã€‘\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "print(\" æ³¨æ„ï¼š\")\n",
    "print(\"  - [CLS]: å¥å­å¼€å§‹æ ‡è®°ï¼ˆID=101ï¼‰\")\n",
    "print(\"  - [SEP]: å¥å­ç»“æŸæ ‡è®°ï¼ˆID=102ï¼‰\")\n",
    "print(\"  - BERTçš„è¾“å…¥å¿…é¡»æœ‰è¿™ä¸¤ä¸ªç‰¹æ®Štoken\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:43:05.853229Z",
     "start_time": "2025-11-03T07:43:05.664228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ–‡æœ¬é•¿åº¦:\n",
      "  æ–‡æœ¬1: 7å­—\n",
      "  æ–‡æœ¬2: 23å­—\n",
      "  æ–‡æœ¬3: 49å­—\n",
      "\n",
      "ã€ç¼–ç ç»“æœã€‘\n",
      "input_ids shape: torch.Size([3, 20])\n",
      "  è§£é‡Š: [batch_size=3, seq_len=20]\n",
      "\n",
      "ã€input_idsã€‘ï¼ˆToken IDsï¼‰:\n",
      "tensor([[ 101, 6821, 3221,  671, 3340, 4764, 3173, 7319,  102,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 6821, 3221,  671, 3340, 4924, 2544, 7270,  671, 4157, 4638, 3173,\n",
      "         7319, 8024, 1259, 1419, 3291, 1914, 4638,  102],\n",
      "        [ 101, 6821, 3221,  671, 3340, 2523, 7270, 2523, 7270, 4638, 3173, 7319,\n",
      "         8024, 1259, 1419,  749, 7478, 2382, 1914,  102]])\n",
      "\n",
      "ã€attention_maskã€‘ï¼ˆæ³¨æ„åŠ›æ©ç ï¼‰:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "  è§£é‡Š: 1=çœŸå®token, 0=padding\n",
      "\n",
      "å…³é”®ç†è§£:\n",
      "  - padding: çŸ­æ–‡æœ¬ç”¨[PAD](ID=0)è¡¥é½\n",
      "  - attention_mask: å‘Šè¯‰BERTå¿½ç•¥paddingéƒ¨åˆ†\n",
      "  - truncation: é•¿æ–‡æœ¬æˆªæ–­åˆ°max_length\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Tokenizeré«˜çº§ç”¨æ³• - paddingå’Œtruncation\n",
    "\"\"\"\n",
    "å®é™…è®­ç»ƒæ—¶éœ€è¦çš„åŠŸèƒ½ï¼š\n",
    "- padding: å°†çŸ­æ–‡æœ¬è¡¥é½åˆ°å›ºå®šé•¿åº¦\n",
    "- truncation: å°†é•¿æ–‡æœ¬æˆªæ–­\n",
    "- attention_mask: å‘Šè¯‰BERTå“ªäº›æ˜¯çœŸå®å†…å®¹ï¼Œå“ªäº›æ˜¯padding\n",
    "\"\"\"\n",
    "\n",
    "# å‡†å¤‡å¤šæ¡æ–‡æœ¬ï¼ˆé•¿åº¦ä¸ä¸€ï¼‰\n",
    "texts = [\n",
    "    \"è¿™æ˜¯ä¸€æ¡çŸ­æ–°é—»\",  # çŸ­\n",
    "    \"è¿™æ˜¯ä¸€æ¡ç¨å¾®é•¿ä¸€ç‚¹çš„æ–°é—»ï¼ŒåŒ…å«æ›´å¤šçš„ä¿¡æ¯å’Œå†…å®¹\",  # ä¸­\n",
    "    \"è¿™æ˜¯ä¸€æ¡å¾ˆé•¿å¾ˆé•¿çš„æ–°é—»ï¼ŒåŒ…å«äº†éå¸¸å¤šçš„è¯¦ç»†ä¿¡æ¯ï¼Œæè¿°äº†äº‹ä»¶çš„æ¥é¾™å»è„‰ï¼Œä»¥åŠç›¸å…³çš„èƒŒæ™¯çŸ¥è¯†å’Œä¸“å®¶åˆ†æ\"  # é•¿\n",
    "]\n",
    "\n",
    "print(\"åŸå§‹æ–‡æœ¬é•¿åº¦:\")\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"  æ–‡æœ¬{i+1}: {len(text)}å­—\")\n",
    "\n",
    "# ä½¿ç”¨tokenizerçš„é«˜çº§åŠŸèƒ½\n",
    "encoding = tokenizer(\n",
    "    texts,\n",
    "    padding=True,        # è‡ªåŠ¨paddingåˆ°batchä¸­æœ€é•¿çš„é•¿åº¦\n",
    "    truncation=True,     # è¶…è¿‡max_lengthå°±æˆªæ–­\n",
    "    max_length=20,       # æœ€å¤§é•¿åº¦è®¾ä¸º20ï¼ˆæ¼”ç¤ºç”¨ï¼‰\n",
    "    return_tensors='pt'  # è¿”å›PyTorch tensor\n",
    ")\n",
    "\n",
    "print(f\"\\nã€ç¼–ç ç»“æœã€‘\")\n",
    "print(f\"input_ids shape: {encoding['input_ids'].shape}\")\n",
    "print(f\"  è§£é‡Š: [batch_size=3, seq_len=20]\")\n",
    "\n",
    "print(f\"\\nã€input_idsã€‘ï¼ˆToken IDsï¼‰:\")\n",
    "print(encoding['input_ids'])\n",
    "\n",
    "print(f\"\\nã€attention_maskã€‘ï¼ˆæ³¨æ„åŠ›æ©ç ï¼‰:\")\n",
    "print(encoding['attention_mask'])\n",
    "print(\"  è§£é‡Š: 1=çœŸå®token, 0=padding\")\n",
    "\n",
    "print(f\"\\nå…³é”®ç†è§£:\")\n",
    "print(\"  - padding: çŸ­æ–‡æœ¬ç”¨[PAD](ID=0)è¡¥é½\")\n",
    "print(\"  - attention_mask: å‘Šè¯‰BERTå¿½ç•¥paddingéƒ¨åˆ†\")\n",
    "print(\"  - truncation: é•¿æ–‡æœ¬æˆªæ–­åˆ°max_length\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:51:16.635982Z",
     "start_time": "2025-11-03T07:49:19.502547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½ bert-base-chinese æ¨¡å‹...\n",
      "ï¼ˆç¬¬ä¸€æ¬¡ä¼šä¸‹è½½ï¼Œçº¦400MBï¼Œéœ€è¦å‡ åˆ†é’Ÿï¼‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-chinese/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C421DF7850>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 6a14f698-bd4c-4081-b4fe-cea1b3cc8f22)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-chinese/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C42755E620>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: c537e7cc-f9a8-4892-97d7-13dc6b0e7410)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269dbfd14c6645d39354d0fce971f812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTæ¨¡å‹åŠ è½½æˆåŠŸ\n",
      "\n",
      "æ¨¡å‹å‚æ•°é‡: 102.3M\n",
      "  - BERTå±‚: ~110Må‚æ•°ï¼ˆé¢„è®­ç»ƒå¥½çš„ï¼‰\n",
      "  - åˆ†ç±»å±‚: ~11Kå‚æ•°ï¼ˆéœ€è¦è®­ç»ƒï¼‰\n",
      "\n",
      "æ¨¡å‹ç»“æ„æ¦‚è§ˆ:\n",
      "  è¾“å…¥: [batch, seq_len] çš„token IDs\n",
      "  BERTç¼–ç : â†’ [batch, seq_len, 768]\n",
      "  [CLS]è¡¨ç¤º: â†’ [batch, 768]\n",
      "  åˆ†ç±»å±‚: â†’ [batch, 14]\n",
      "  è¾“å‡º: 14ä¸ªç±»åˆ«çš„åˆ†æ•°\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: åŠ è½½BERTåˆ†ç±»æ¨¡å‹\n",
    "\"\"\"\n",
    "BertForSequenceClassificationï¼š\n",
    "- BERTæ¨¡å‹ + åˆ†ç±»å¤´ï¼ˆå…¨è¿æ¥å±‚ï¼‰\n",
    "- é¢„è®­ç»ƒå¥½çš„BERT + éšæœºåˆå§‹åŒ–çš„åˆ†ç±»å±‚\n",
    "- å¾®è°ƒæ—¶ï¼šBERTå‚æ•°ä¼šæ›´æ–°ï¼Œåˆ†ç±»å±‚ä»é›¶å­¦ä¹ \n",
    "\"\"\"\n",
    "\n",
    "print(f\"æ­£åœ¨åŠ è½½ {model_name} æ¨¡å‹...\")\n",
    "print(\"ï¼ˆç¬¬ä¸€æ¬¡ä¼šä¸‹è½½ï¼Œçº¦400MBï¼Œéœ€è¦å‡ åˆ†é’Ÿï¼‰\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹ï¼ˆæŒ‡å®šåˆ†ç±»ç±»åˆ«æ•°ï¼‰\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=14  # THUCNewsæœ‰14ä¸ªç±»åˆ«\n",
    ")\n",
    "\n",
    "print(\"BERTæ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "\n",
    "# æ¨¡å‹ä¿¡æ¯\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\næ¨¡å‹å‚æ•°é‡: {total_params/1e6:.1f}M\")\n",
    "print(f\"  - BERTå±‚: ~110Må‚æ•°ï¼ˆé¢„è®­ç»ƒå¥½çš„ï¼‰\")\n",
    "print(f\"  - åˆ†ç±»å±‚: ~11Kå‚æ•°ï¼ˆéœ€è¦è®­ç»ƒï¼‰\")\n",
    "\n",
    "print(f\"\\næ¨¡å‹ç»“æ„æ¦‚è§ˆ:\")\n",
    "print(f\"  è¾“å…¥: [batch, seq_len] çš„token IDs\")\n",
    "print(f\"  BERTç¼–ç : â†’ [batch, seq_len, 768]\")\n",
    "print(f\"  [CLS]è¡¨ç¤º: â†’ [batch, 768]\")\n",
    "print(f\"  åˆ†ç±»å±‚: â†’ [batch, 14]\")\n",
    "print(f\"  è¾“å‡º: 14ä¸ªç±»åˆ«çš„åˆ†æ•°\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T08:10:57.827470Z",
     "start_time": "2025-11-03T08:10:54.357600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æµ‹è¯•æ–‡æœ¬: è‚¡å¸‚ä»Šæ—¥å¤§æ¶¨ï¼Œä¸Šè¯æŒ‡æ•°çªç ´3000ç‚¹\n",
      "\n",
      "ç¼–ç åçš„æ ¼å¼:\n",
      "  input_ids shape: torch.Size([1, 128])\n",
      "  attention_mask shape: torch.Size([1, 128])\n",
      "\n",
      "ã€æ¨¡å‹è¾“å‡ºã€‘\n",
      "outputsç±»å‹: <class 'transformers.modeling_outputs.SequenceClassifierOutput'>\n",
      "logits shape: torch.Size([1, 14])\n",
      "  è§£é‡Š: [batch_size=1, num_labels=14]\n",
      "\n",
      "ã€é¢„æµ‹ç»“æœã€‘ï¼ˆæœªè®­ç»ƒï¼Œéšæœºçš„ï¼‰\n",
      "é¢„æµ‹ç±»åˆ«: 11\n",
      "æœ€é«˜æ¦‚ç‡: 0.1599\n",
      "\n",
      "å„ç±»åˆ«æ¦‚ç‡ï¼ˆå‰5ä¸ªï¼‰:\n",
      "  ç±»åˆ«0: 0.0206\n",
      "  ç±»åˆ«1: 0.0762\n",
      "  ç±»åˆ«2: 0.0517\n",
      "  ç±»åˆ«3: 0.1190\n",
      "  ç±»åˆ«4: 0.0658\n",
      "\n",
      " æ³¨æ„ï¼š\n",
      "  è¿™æ˜¯æœªè®­ç»ƒçš„æ¨¡å‹ï¼Œé¢„æµ‹æ˜¯éšæœºçš„\n",
      "  å¾®è°ƒåï¼Œæ¨¡å‹ä¼šå­¦ä¼šæ­£ç¡®åˆ†ç±»\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: æµ‹è¯•æ¨¡å‹æ¨ç†ï¼ˆæœªè®­ç»ƒçš„æ¨¡å‹ï¼‰\n",
    "\"\"\"\n",
    "æµ‹è¯•æœªç»å¾®è°ƒçš„BERTèƒ½å¦è¿è¡Œ\n",
    "æ³¨æ„ï¼šæ­¤æ—¶åˆ†ç±»å±‚æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œé¢„æµ‹ç»“æœæ˜¯éšæœºçš„\n",
    "å¾®è°ƒåæ‰ä¼šæœ‰å‡†ç¡®çš„é¢„æµ‹\n",
    "\"\"\"\n",
    "\n",
    "# å‡†å¤‡è¾“å…¥\n",
    "test_text = \"è‚¡å¸‚ä»Šæ—¥å¤§æ¶¨ï¼Œä¸Šè¯æŒ‡æ•°çªç ´3000ç‚¹\"\n",
    "\n",
    "print(f\"æµ‹è¯•æ–‡æœ¬: {test_text}\")\n",
    "\n",
    "# Tokenize\n",
    "encoding = tokenizer(\n",
    "    test_text,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=128,  # å®é™…è®­ç»ƒæ—¶å¯èƒ½ç”¨512\n",
    "    return_tensors='pt'  # è¿”å›PyTorch tensor\n",
    ")\n",
    "\n",
    "print(f\"\\nç¼–ç åçš„æ ¼å¼:\")\n",
    "print(f\"  input_ids shape: {encoding['input_ids'].shape}\")\n",
    "print(f\"  attention_mask shape: {encoding['attention_mask'].shape}\")\n",
    "\n",
    "# æ¨¡å‹æ¨ç†\n",
    "model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ˆæ¨ç†æ—¶ï¼‰\n",
    "    outputs = model(**encoding)  # **è¡¨ç¤ºè§£åŒ…å­—å…¸ä½œä¸ºå‚æ•°\n",
    "    # ç­‰ä»·äºï¼šmodel(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
    "\n",
    "# æŸ¥çœ‹è¾“å‡º\n",
    "print(f\"\\nã€æ¨¡å‹è¾“å‡ºã€‘\")\n",
    "print(f\"outputsç±»å‹: {type(outputs)}\")\n",
    "print(f\"logits shape: {outputs.logits.shape}\")\n",
    "print(f\"  è§£é‡Š: [batch_size=1, num_labels=14]\")\n",
    "\n",
    "# è·å–é¢„æµ‹ç±»åˆ«\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "probs = torch.softmax(logits, dim=1)[0]  # è½¬ä¸ºæ¦‚ç‡\n",
    "\n",
    "print(f\"\\nã€é¢„æµ‹ç»“æœã€‘ï¼ˆæœªè®­ç»ƒï¼Œéšæœºçš„ï¼‰\")\n",
    "print(f\"é¢„æµ‹ç±»åˆ«: {predicted_class}\")\n",
    "print(f\"æœ€é«˜æ¦‚ç‡: {probs[predicted_class]:.4f}\")\n",
    "print(f\"\\nå„ç±»åˆ«æ¦‚ç‡ï¼ˆå‰5ä¸ªï¼‰:\")\n",
    "for i in range(5):\n",
    "    print(f\"  ç±»åˆ«{i}: {probs[i]:.4f}\")\n",
    "\n",
    "print(f\"\\n æ³¨æ„ï¼š\")\n",
    "print(f\"  è¿™æ˜¯æœªè®­ç»ƒçš„æ¨¡å‹ï¼Œé¢„æµ‹æ˜¯éšæœºçš„\")\n",
    "print(f\"  å¾®è°ƒåï¼Œæ¨¡å‹ä¼šå­¦ä¼šæ­£ç¡®åˆ†ç±»\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T08:12:32.915117Z",
     "start_time": "2025-11-03T08:12:32.232539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BERT vs ä¼ ç»Ÿæ–¹æ³•ï¼ˆTextCNN/LSTMï¼‰å¯¹æ¯”\n",
      "============================================================\n",
      "\n",
      "ã€ä¼ ç»Ÿæ–¹æ³•ï¼ˆTextCNN/LSTMï¼‰ã€‘\n",
      "  1. åˆ†è¯: jiebaåˆ†è¯\n",
      "  2. æ„å»ºè¯è¡¨: ç»Ÿè®¡è¯é¢‘ï¼Œåˆ†é…ID\n",
      "  3. æ–‡æœ¬â†’ID: æŸ¥è¯è¡¨ï¼ŒæœªçŸ¥è¯ç”¨<UNK>\n",
      "  4. Embedding: ä»é›¶å­¦ä¹ è¯å‘é‡ï¼ˆæˆ–ç”¨Word2Vecï¼‰\n",
      "  ç¼ºç‚¹: è¯è¡¨éœ€è¦è‡ªå·±æ„å»ºï¼ŒæœªçŸ¥è¯å¤„ç†ä¸å¥½\n",
      "\n",
      "ã€BERTæ–¹æ³•ã€‘\n",
      "  1. åˆ†è¯: BertTokenizerï¼ˆå­—ç¬¦çº§ï¼‰\n",
      "  2. è¯è¡¨: ä½¿ç”¨é¢„è®­ç»ƒçš„å›ºå®šè¯è¡¨ï¼ˆ21128ä¸ªï¼‰\n",
      "  3. æ–‡æœ¬â†’ID: ç›´æ¥ç”¨tokenizer\n",
      "  4. Embedding: ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡ï¼ˆå·²ç»å­¦å¥½çš„ï¼‰\n",
      "  ä¼˜ç‚¹: ä¸éœ€è¦æ„å»ºè¯è¡¨ï¼Œé¢„è®­ç»ƒembeddingæ•ˆæœå¥½\n",
      "\n",
      "ã€å…·ä½“ç¤ºä¾‹ã€‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ysn\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.434 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "jiebaåˆ†è¯: ['äººå·¥æ™ºèƒ½', 'å’Œ', 'æ·±åº¦', 'å­¦ä¹ ']\n",
      "  éœ€è¦: è‡ªå·±æ„å»ºè¯è¡¨ï¼Œ'äººå·¥æ™ºèƒ½'å¯èƒ½æ˜¯æœªçŸ¥è¯\n",
      "\n",
      "BERTåˆ†è¯: ['äºº', 'å·¥', 'æ™º', 'èƒ½', 'å’Œ', 'æ·±', 'åº¦', 'å­¦', 'ä¹ ']\n",
      "  è¯´æ˜: å­—ç¬¦çº§åˆ†è¯ï¼Œæ¯ä¸ªå­—éƒ½è®¤è¯†\n",
      "  ä¼˜åŠ¿: æ²¡æœ‰æœªçŸ¥è¯é—®é¢˜\n",
      "\n",
      " æ ¸å¿ƒåŒºåˆ«:\n",
      "  - ä¼ ç»Ÿæ–¹æ³•: éœ€è¦è‡ªå·±å»ºè¯è¡¨ï¼ˆå·¥ä½œé‡å¤§ï¼‰\n",
      "  - BERT: ç›´æ¥ç”¨é¢„è®­ç»ƒçš„ï¼ˆçœäº‹ï¼ï¼‰\n",
      "  - BERTçš„embeddingå·²ç»åœ¨å¤§é‡æ–‡æœ¬ä¸Šå­¦ä¹ è¿‡ï¼Œæ•ˆæœæ›´å¥½\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: BERT vs TextCNN/LSTM - æ•°æ®å¤„ç†å¯¹æ¯”\n",
    "\"\"\"\n",
    "ç†è§£BERTå’Œä¼ ç»Ÿæ–¹æ³•çš„åŒºåˆ«\n",
    "å…³é”®ï¼šBERTä¸éœ€è¦è‡ªå·±æ„å»ºè¯è¡¨ï¼\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BERT vs ä¼ ç»Ÿæ–¹æ³•ï¼ˆTextCNN/LSTMï¼‰å¯¹æ¯”\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nã€ä¼ ç»Ÿæ–¹æ³•ï¼ˆTextCNN/LSTMï¼‰ã€‘\")\n",
    "print(\"  1. åˆ†è¯: jiebaåˆ†è¯\")\n",
    "print(\"  2. æ„å»ºè¯è¡¨: ç»Ÿè®¡è¯é¢‘ï¼Œåˆ†é…ID\")\n",
    "print(\"  3. æ–‡æœ¬â†’ID: æŸ¥è¯è¡¨ï¼ŒæœªçŸ¥è¯ç”¨<UNK>\")\n",
    "print(\"  4. Embedding: ä»é›¶å­¦ä¹ è¯å‘é‡ï¼ˆæˆ–ç”¨Word2Vecï¼‰\")\n",
    "print(\"  ç¼ºç‚¹: è¯è¡¨éœ€è¦è‡ªå·±æ„å»ºï¼ŒæœªçŸ¥è¯å¤„ç†ä¸å¥½\")\n",
    "\n",
    "print(\"\\nã€BERTæ–¹æ³•ã€‘\")\n",
    "print(\"  1. åˆ†è¯: BertTokenizerï¼ˆå­—ç¬¦çº§ï¼‰\")\n",
    "print(\"  2. è¯è¡¨: ä½¿ç”¨é¢„è®­ç»ƒçš„å›ºå®šè¯è¡¨ï¼ˆ21128ä¸ªï¼‰\")\n",
    "print(\"  3. æ–‡æœ¬â†’ID: ç›´æ¥ç”¨tokenizer\")\n",
    "print(\"  4. Embedding: ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡ï¼ˆå·²ç»å­¦å¥½çš„ï¼‰\")\n",
    "print(\"  ä¼˜ç‚¹: ä¸éœ€è¦æ„å»ºè¯è¡¨ï¼Œé¢„è®­ç»ƒembeddingæ•ˆæœå¥½\")\n",
    "\n",
    "print(\"\\nã€å…·ä½“ç¤ºä¾‹ã€‘\")\n",
    "text = \"äººå·¥æ™ºèƒ½å’Œæ·±åº¦å­¦ä¹ \"\n",
    "\n",
    "# ä¼ ç»Ÿæ–¹æ³•ï¼ˆjiebaï¼‰\n",
    "import jieba\n",
    "jieba_tokens = list(jieba.cut(text))\n",
    "print(f\"\\njiebaåˆ†è¯: {jieba_tokens}\")\n",
    "print(f\"  éœ€è¦: è‡ªå·±æ„å»ºè¯è¡¨ï¼Œ'äººå·¥æ™ºèƒ½'å¯èƒ½æ˜¯æœªçŸ¥è¯\")\n",
    "\n",
    "# BERTæ–¹æ³•\n",
    "bert_tokens = tokenizer.tokenize(text)\n",
    "print(f\"\\nBERTåˆ†è¯: {bert_tokens}\")\n",
    "print(f\"  è¯´æ˜: å­—ç¬¦çº§åˆ†è¯ï¼Œæ¯ä¸ªå­—éƒ½è®¤è¯†\")\n",
    "print(f\"  ä¼˜åŠ¿: æ²¡æœ‰æœªçŸ¥è¯é—®é¢˜\")\n",
    "\n",
    "print(\"\\n æ ¸å¿ƒåŒºåˆ«:\")\n",
    "print(\"  - ä¼ ç»Ÿæ–¹æ³•: éœ€è¦è‡ªå·±å»ºè¯è¡¨ï¼ˆå·¥ä½œé‡å¤§ï¼‰\")\n",
    "print(\"  - BERT: ç›´æ¥ç”¨é¢„è®­ç»ƒçš„ï¼ˆçœäº‹ï¼ï¼‰\")\n",
    "print(\"  - BERTçš„embeddingå·²ç»åœ¨å¤§é‡æ–‡æœ¬ä¸Šå­¦ä¹ è¿‡ï¼Œæ•ˆæœæ›´å¥½\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T08:13:11.428210Z",
     "start_time": "2025-11-03T08:13:11.204771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batchæ•°æ®:\n",
      "  1. ä¸­å›½ç”·ç¯®åœ¨ä¸–ç•Œæ¯ä¸Šå–å¾—ä½³ç»©\n",
      "  2. è‚¡å¸‚ä»Šæ—¥æ”¶ç›˜ï¼Œä¸Šè¯æŒ‡æ•°ä¸Šæ¶¨\n",
      "  3. æœ€æ–°æ™ºèƒ½æ‰‹æœºå‘å¸ƒï¼Œæ­è½½AIèŠ¯ç‰‡\n",
      "\n",
      "Batchç¼–ç :\n",
      "  input_ids shape: torch.Size([3, 128])\n",
      "  è§£é‡Š: [batch_size=3, seq_len=128]\n",
      "\n",
      "ã€é¢„æµ‹ç»“æœã€‘ï¼ˆæœªè®­ç»ƒï¼Œéšæœºï¼‰\n",
      "é¢„æµ‹ç±»åˆ«: tensor([ 3, 11, 10])\n",
      "çœŸå®ç±»åˆ«: tensor([ 0, 13, 11])\n",
      "\n",
      "Loss: 2.8547\n",
      "  è§£é‡Š: è¿™æ˜¯æœªè®­ç»ƒæ¨¡å‹çš„lossï¼Œå¾ˆå¤§æ˜¯æ­£å¸¸çš„\n",
      "  å¾®è°ƒålossä¼šé™ä½ï¼Œå‡†ç¡®ç‡ä¼šæå‡\n",
      "\n",
      " Batchå¤„ç†æµ‹è¯•é€šè¿‡\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Batchå¤„ç†æµ‹è¯•\n",
    "\"\"\"\n",
    "æµ‹è¯•BERTå¤„ç†ä¸€ä¸ªbatchçš„æ•°æ®\n",
    "æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„æƒ…å†µ\n",
    "\"\"\"\n",
    "\n",
    "# å‡†å¤‡ä¸€ä¸ªbatchçš„æ–‡æœ¬ï¼ˆ3æ¡æ–°é—»ï¼‰\n",
    "batch_texts = [\n",
    "    \"ä¸­å›½ç”·ç¯®åœ¨ä¸–ç•Œæ¯ä¸Šå–å¾—ä½³ç»©\",  # ä½“è‚²\n",
    "    \"è‚¡å¸‚ä»Šæ—¥æ”¶ç›˜ï¼Œä¸Šè¯æŒ‡æ•°ä¸Šæ¶¨\",  # è´¢ç»\n",
    "    \"æœ€æ–°æ™ºèƒ½æ‰‹æœºå‘å¸ƒï¼Œæ­è½½AIèŠ¯ç‰‡\"   # ç§‘æŠ€\n",
    "]\n",
    "\n",
    "batch_labels = torch.tensor([0, 13, 11])  # å‡è®¾çš„çœŸå®æ ‡ç­¾\n",
    "\n",
    "print(\"Batchæ•°æ®:\")\n",
    "for i, text in enumerate(batch_texts):\n",
    "    print(f\"  {i+1}. {text}\")\n",
    "\n",
    "# Tokenizeæ•´ä¸ªbatch\n",
    "batch_encoding = tokenizer(\n",
    "    batch_texts,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(f\"\\nBatchç¼–ç :\")\n",
    "print(f\"  input_ids shape: {batch_encoding['input_ids'].shape}\")\n",
    "print(f\"  è§£é‡Š: [batch_size=3, seq_len=128]\")\n",
    "\n",
    "# æ¨¡å‹æ¨ç†\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch_encoding)\n",
    "    logits = outputs.logits  # [3, 14]\n",
    "\n",
    "# è·å–é¢„æµ‹\n",
    "predicted_classes = torch.argmax(logits, dim=1)\n",
    "print(f\"\\nã€é¢„æµ‹ç»“æœã€‘ï¼ˆæœªè®­ç»ƒï¼Œéšæœºï¼‰\")\n",
    "print(f\"é¢„æµ‹ç±»åˆ«: {predicted_classes}\")\n",
    "print(f\"çœŸå®ç±»åˆ«: {batch_labels}\")\n",
    "\n",
    "# å¦‚æœè®­ç»ƒè¿‡ï¼Œå¯ä»¥è®¡ç®—loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(logits, batch_labels)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n",
    "print(f\"  è§£é‡Š: è¿™æ˜¯æœªè®­ç»ƒæ¨¡å‹çš„lossï¼Œå¾ˆå¤§æ˜¯æ­£å¸¸çš„\")\n",
    "print(f\"  å¾®è°ƒålossä¼šé™ä½ï¼Œå‡†ç¡®ç‡ä¼šæå‡\")\n",
    "\n",
    "print(\"\\n Batchå¤„ç†æµ‹è¯•é€šè¿‡\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T08:13:14.442036Z",
     "start_time": "2025-11-03T08:13:14.422007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ‰ BERTæµ‹è¯•å®Œæˆï¼æ ¸å¿ƒçŸ¥è¯†æ€»ç»“\n",
      "============================================================\n",
      "\n",
      "ã€1. BERTæ ¸å¿ƒç»„ä»¶ã€‘\n",
      "   BertTokenizer: æ–‡æœ¬â†’Token IDs\n",
      "   BertForSequenceClassification: BERT+åˆ†ç±»å±‚\n",
      "   é¢„è®­ç»ƒæ¨¡å‹: å·²ç»åœ¨å¤§é‡æ–‡æœ¬ä¸Šå­¦ä¹ è¿‡\n",
      "\n",
      "ã€2. BERTä½¿ç”¨æµç¨‹ã€‘\n",
      "  Step 1: åŠ è½½tokenizerå’Œæ¨¡å‹\n",
      "  Step 2: tokenizerå¤„ç†æ–‡æœ¬\n",
      "     - åˆ†è¯ã€è½¬IDã€paddingã€attention_mask\n",
      "  Step 3: æ¨¡å‹æ¨ç†/è®­ç»ƒ\n",
      "     - è¾“å…¥: input_ids + attention_mask\n",
      "     - è¾“å‡º: logitsï¼ˆ14ä¸ªç±»åˆ«åˆ†æ•°ï¼‰\n",
      "  Step 4: è·å–é¢„æµ‹\n",
      "     - torch.argmax(logits, dim=1)\n",
      "\n",
      "ã€3. BERT vs ä¼ ç»Ÿæ–¹æ³•ã€‘\n",
      "  ä¼ ç»Ÿæ–¹æ³•:\n",
      "    - éœ€è¦: åˆ†è¯ã€å»ºè¯è¡¨ã€è®­ç»ƒembedding\n",
      "    - ä¼˜ç‚¹: çµæ´»ã€å¯æ§\n",
      "    - ç¼ºç‚¹: å·¥ä½œé‡å¤§ã€æœªçŸ¥è¯é—®é¢˜\n",
      "  \n",
      "  BERTæ–¹æ³•:\n",
      "    - éœ€è¦: ç›´æ¥ç”¨tokenizer\n",
      "    - ä¼˜ç‚¹: ç®€å•ã€é¢„è®­ç»ƒembeddingæ•ˆæœå¥½\n",
      "    - ç¼ºç‚¹: æ¨¡å‹å¤§ã€æ¨ç†æ…¢\n",
      "\n",
      "ã€5. å…³é”®å‚æ•°è®°å½•ã€‘\n",
      "  - æ¨¡å‹: bert-base-chinese\n",
      "  - è¯è¡¨å¤§å°: 21128\n",
      "  - å‚æ•°é‡: ~110M\n",
      "  - Max length: 512ï¼ˆå®é™…è®­ç»ƒæ—¶ï¼‰\n",
      "  - ç±»åˆ«æ•°: 14\n",
      "\n",
      " ç¯å¢ƒéªŒè¯å®Œæˆï¼å‡†å¤‡å¼€å§‹BERTå¾®è°ƒï¼\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: æ€»ç»“ - BERTä½¿ç”¨æµç¨‹\n",
    "\"\"\"\n",
    "æ€»ç»“BERTçš„å®Œæ•´ä½¿ç”¨æµç¨‹\n",
    "ä¸ºæ˜å¤©çš„å¾®è°ƒè®­ç»ƒåšå‡†å¤‡\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\" BERTæµ‹è¯•å®Œæˆï¼æ ¸å¿ƒçŸ¥è¯†æ€»ç»“\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nã€1. BERTæ ¸å¿ƒç»„ä»¶ã€‘\")\n",
    "print(\"   BertTokenizer: æ–‡æœ¬â†’Token IDs\")\n",
    "print(\"   BertForSequenceClassification: BERT+åˆ†ç±»å±‚\")\n",
    "print(\"   é¢„è®­ç»ƒæ¨¡å‹: å·²ç»åœ¨å¤§é‡æ–‡æœ¬ä¸Šå­¦ä¹ è¿‡\")\n",
    "\n",
    "print(\"\\nã€2. BERTä½¿ç”¨æµç¨‹ã€‘\")\n",
    "print(\"  Step 1: åŠ è½½tokenizerå’Œæ¨¡å‹\")\n",
    "print(\"  Step 2: tokenizerå¤„ç†æ–‡æœ¬\")\n",
    "print(\"     - åˆ†è¯ã€è½¬IDã€paddingã€attention_mask\")\n",
    "print(\"  Step 3: æ¨¡å‹æ¨ç†/è®­ç»ƒ\")\n",
    "print(\"     - è¾“å…¥: input_ids + attention_mask\")\n",
    "print(\"     - è¾“å‡º: logitsï¼ˆ14ä¸ªç±»åˆ«åˆ†æ•°ï¼‰\")\n",
    "print(\"  Step 4: è·å–é¢„æµ‹\")\n",
    "print(\"     - torch.argmax(logits, dim=1)\")\n",
    "\n",
    "print(\"\\nã€3. BERT vs ä¼ ç»Ÿæ–¹æ³•ã€‘\")\n",
    "print(\"  ä¼ ç»Ÿæ–¹æ³•:\")\n",
    "print(\"    - éœ€è¦: åˆ†è¯ã€å»ºè¯è¡¨ã€è®­ç»ƒembedding\")\n",
    "print(\"    - ä¼˜ç‚¹: çµæ´»ã€å¯æ§\")\n",
    "print(\"    - ç¼ºç‚¹: å·¥ä½œé‡å¤§ã€æœªçŸ¥è¯é—®é¢˜\")\n",
    "print(\"  \")\n",
    "print(\"  BERTæ–¹æ³•:\")\n",
    "print(\"    - éœ€è¦: ç›´æ¥ç”¨tokenizer\")\n",
    "print(\"    - ä¼˜ç‚¹: ç®€å•ã€é¢„è®­ç»ƒembeddingæ•ˆæœå¥½\")\n",
    "print(\"    - ç¼ºç‚¹: æ¨¡å‹å¤§ã€æ¨ç†æ…¢\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nã€5. å…³é”®å‚æ•°è®°å½•ã€‘\")\n",
    "print(f\"  - æ¨¡å‹: bert-base-chinese\")\n",
    "print(f\"  - è¯è¡¨å¤§å°: {tokenizer.vocab_size}\")\n",
    "print(f\"  - å‚æ•°é‡: ~110M\")\n",
    "print(f\"  - Max length: 512ï¼ˆå®é™…è®­ç»ƒæ—¶ï¼‰\")\n",
    "print(f\"  - ç±»åˆ«æ•°: 14\")\n",
    "\n",
    "print(\"\\n ç¯å¢ƒéªŒè¯å®Œæˆï¼å‡†å¤‡å¼€å§‹BERTå¾®è°ƒï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
